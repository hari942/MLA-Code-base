{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Ordinal Encoding for 'size'\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10656 entries, 0 to 10655\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              10656 non-null  int64  \n",
      " 1   area_type       10656 non-null  object \n",
      " 2   availability    10656 non-null  object \n",
      " 3   location        10655 non-null  object \n",
      " 4   size            10642 non-null  object \n",
      " 5   society         6228 non-null   object \n",
      " 6   total_sqft      10656 non-null  object \n",
      " 7   bath            10591 non-null  float64\n",
      " 8   balcony         10152 non-null  float64\n",
      " 9   price           10656 non-null  float64\n",
      " 10  dist_from_city  9630 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 915.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2664 entries, 0 to 2663\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              2664 non-null   int64  \n",
      " 1   area_type       2664 non-null   object \n",
      " 2   availability    2664 non-null   object \n",
      " 3   location        2664 non-null   object \n",
      " 4   size            2662 non-null   object \n",
      " 5   society         1590 non-null   object \n",
      " 6   total_sqft      2664 non-null   object \n",
      " 7   bath            2656 non-null   float64\n",
      " 8   balcony         2559 non-null   float64\n",
      " 9   dist_from_city  2390 non-null   float64\n",
      "dtypes: float64(3), int64(1), object(6)\n",
      "memory usage: 208.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# read dist_from_city_center.csv \n",
    "dist = pd.read_csv('dist_from_city_centre.csv')\n",
    "dist.count()\n",
    "\n",
    "# merge train and dist data frames on location\n",
    "train_dataset = pd.merge(train_df, dist, on='location', how='left')\n",
    "train_dataset.head()\n",
    "test_dataset = pd.merge(test_df, dist, on='location', how='left')\n",
    "train_dataset.info()\n",
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10656 entries, 0 to 10655\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ID              10656 non-null  int64  \n",
      " 1   area_type       10656 non-null  object \n",
      " 2   availability    10656 non-null  object \n",
      " 3   location        10655 non-null  object \n",
      " 4   size            10642 non-null  object \n",
      " 5   society         6228 non-null   object \n",
      " 6   total_sqft      10656 non-null  object \n",
      " 7   bath            10591 non-null  float64\n",
      " 8   balcony         10152 non-null  float64\n",
      " 9   price           10656 non-null  float64\n",
      " 10  dist_from_city  9630 non-null   float64\n",
      " 11  avg_2bhk_rent   3665 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(6)\n",
      "memory usage: 999.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# lets add avg rent from avg rent csv\n",
    "avg_rent = pd.read_csv('avg_rent.csv')\n",
    "\n",
    "# Lets merge train and avg_rent on location\n",
    "\n",
    "train_dataset = pd.merge(train_dataset, avg_rent, on='location', how='left')\n",
    "test_dataset = pd.merge(test_dataset, avg_rent, on='location', how='left')\n",
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10616.000000\n",
       "mean      1559.791263\n",
       "std       1273.981228\n",
       "min          1.000000\n",
       "25%       1100.000000\n",
       "50%       1275.000000\n",
       "75%       1680.000000\n",
       "max      52272.000000\n",
       "Name: total_sqft, dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse total_sqft\n",
    "train_dataset['total_sqft'].describe()\n",
    "\n",
    "# convert to sqft\n",
    "\n",
    "# Sq Meter to sqft convert\n",
    "def sqmt_to_sqft(x):\n",
    "    # if x is string and contains Sq. Meter\n",
    "    if type(x) == str and 'Sq. Meter' in x:\n",
    "        return float(x.split('Sq. Meter')[0]) * 10.7639\n",
    "    return x\n",
    "\n",
    "# Sq Yards to sqft convert\n",
    "def sqyd_to_sqft(x):\n",
    "    if type(x) == str and 'Sq. Yards' in x:\n",
    "        return float(x.split('Sq. Yards')[0]) * 9\n",
    "    return x\n",
    "\n",
    "# Perch to sqft convert\n",
    "def perch_to_sqft(x):\n",
    "    if type(x) == str and 'Perch' in x:\n",
    "        return float(x.split('Perch')[0]) * 272.25\n",
    "    return x\n",
    "\n",
    "# Acres to sqft convert\n",
    "def acres_to_sqft(x):\n",
    "    if type(x) == str and 'Acres' in x:\n",
    "        return float(x.split('Acres')[0]) * 43560\n",
    "    return x\n",
    "\n",
    "# Cents to sqft convert\n",
    "def cents_to_sqft(x):\n",
    "    if type(x) == str and 'Cents' in x:\n",
    "        return float(x.split('Cents')[0]) * 435.6\n",
    "    return x\n",
    "\n",
    "# Guntha to sqft convert\n",
    "def guntha_to_sqft(x):\n",
    "    if type(x) == str and 'Guntha' in x:\n",
    "        return float(x.split('Guntha')[0]) * 1089\n",
    "    return x\n",
    "\n",
    "# Grounds to sqft convert\n",
    "def grounds_to_sqft(x):\n",
    "    if type(x) == str and 'Grounds' in x:\n",
    "        return float(x.split('Grounds')[0]) * 2400\n",
    "    return x\n",
    "\n",
    "# convert to sqft\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(sqmt_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(sqyd_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(perch_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(acres_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(cents_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(guntha_to_sqft)\n",
    "train_dataset['total_sqft'] = train_dataset['total_sqft'].apply(grounds_to_sqft)\n",
    "\n",
    "train_dataset['total_sqft'].describe()\n",
    "\n",
    "train_dataset['total_sqft_min'] = train_dataset['total_sqft'].str.split('-').str[0]\n",
    "train_dataset['total_sqft_max'] = train_dataset['total_sqft'].str.split('-').str[1] \n",
    "\n",
    "# fill max with min if max is nan\n",
    "train_dataset['total_sqft_max'] = train_dataset['total_sqft_max'].fillna(train_dataset['total_sqft_min'])\n",
    "\n",
    "# get avg of max and min\n",
    "train_dataset['total_sqft'] = (train_dataset['total_sqft_min'].astype(float) + train_dataset['total_sqft_max'].astype(float))/2\n",
    "\n",
    "train_dataset['total_sqft'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2658.000000\n",
       "mean      1558.969409\n",
       "std       1084.960472\n",
       "min         60.000000\n",
       "25%       1115.000000\n",
       "50%       1290.000000\n",
       "75%       1683.250000\n",
       "max      30400.000000\n",
       "Name: total_sqft, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse total_sqft\n",
    "test_dataset['total_sqft'].describe()\n",
    "\n",
    "# convert to sqft\n",
    "\n",
    "# Sq Meter to sqft convert\n",
    "def sqmt_to_sqft(x):\n",
    "    # if x is string and contains Sq. Meter\n",
    "    if type(x) == str and 'Sq. Meter' in x:\n",
    "        return float(x.split('Sq. Meter')[0]) * 10.7639\n",
    "    return x\n",
    "\n",
    "# Sq Yards to sqft convert\n",
    "def sqyd_to_sqft(x):\n",
    "    if type(x) == str and 'Sq. Yards' in x:\n",
    "        return float(x.split('Sq. Yards')[0]) * 9\n",
    "    return x\n",
    "\n",
    "# Perch to sqft convert\n",
    "def perch_to_sqft(x):\n",
    "    if type(x) == str and 'Perch' in x:\n",
    "        return float(x.split('Perch')[0]) * 272.25\n",
    "    return x\n",
    "\n",
    "# Acres to sqft convert\n",
    "def acres_to_sqft(x):\n",
    "    if type(x) == str and 'Acres' in x:\n",
    "        return float(x.split('Acres')[0]) * 43560\n",
    "    return x\n",
    "\n",
    "# Cents to sqft convert\n",
    "def cents_to_sqft(x):\n",
    "    if type(x) == str and 'Cents' in x:\n",
    "        return float(x.split('Cents')[0]) * 435.6\n",
    "    return x\n",
    "\n",
    "# Guntha to sqft convert\n",
    "def guntha_to_sqft(x):\n",
    "    if type(x) == str and 'Guntha' in x:\n",
    "        return float(x.split('Guntha')[0]) * 1089\n",
    "    return x\n",
    "\n",
    "# Grounds to sqft convert\n",
    "def grounds_to_sqft(x):\n",
    "    if type(x) == str and 'Grounds' in x:\n",
    "        return float(x.split('Grounds')[0]) * 2400\n",
    "    return x\n",
    "\n",
    "# convert to sqft\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(sqmt_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(sqyd_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(perch_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(acres_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(cents_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(guntha_to_sqft)\n",
    "test_dataset['total_sqft'] = test_dataset['total_sqft'].apply(grounds_to_sqft)\n",
    "\n",
    "test_dataset['total_sqft'].describe()\n",
    "\n",
    "test_dataset['total_sqft_min'] = test_dataset['total_sqft'].str.split('-').str[0]\n",
    "test_dataset['total_sqft_max'] = test_dataset['total_sqft'].str.split('-').str[1] \n",
    "\n",
    "# fill max with min if max is nan\n",
    "test_dataset['total_sqft_max'] = test_dataset['total_sqft_max'].fillna(test_dataset['total_sqft_min'])\n",
    "\n",
    "# get avg of max and min\n",
    "test_dataset['total_sqft'] = (test_dataset['total_sqft_min'].astype(float) + test_dataset['total_sqft_max'].astype(float))/2\n",
    "\n",
    "test_dataset['total_sqft'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "\n",
    "    location_mode = df['location'].mode()[0]\n",
    "    df['location'].fillna(location_mode, inplace=True)\n",
    "\n",
    "    # Impute 'size' with a default value ('2 BHK')\n",
    "    df['size'].fillna('2 BHK', inplace=True)\n",
    "\n",
    "    # Extract numerical value from 'size'\n",
    "    def extract_bhk(value):\n",
    "        match = re.search(r'\\d+', str(value))\n",
    "        return int(match.group()) if match else None\n",
    "\n",
    "    df['size'] = df['size'].apply(extract_bhk)\n",
    "\n",
    "    df['society'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    bath_median = df['bath'].median()\n",
    "    df['bath'].fillna(bath_median, inplace=True)\n",
    "\n",
    "    balcony_median = df['balcony'].median()\n",
    "    df['balcony'].fillna(balcony_median, inplace=True)\n",
    "    \n",
    "    # Preprocessing for `dist_from_city`\n",
    "    dist_imputer = SimpleImputer(strategy=\"median\")\n",
    "    df['dist_from_city'] = dist_imputer.fit_transform(df[['dist_from_city']])\n",
    "\n",
    "    # Feature Engineering: Binning `dist_from_city`\n",
    "    binner = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "    df['dist_from_city_bin'] = binner.fit_transform(df[['dist_from_city']]).astype(int)\n",
    "\n",
    "    # Preprocessing for `avg_2bhk_rent`\n",
    "    # Add a missing indicator\n",
    "    df['avg_2bhk_rent_missing'] = df['avg_2bhk_rent'].isna().astype(int)\n",
    "\n",
    "    # Impute missing values in `avg_2bhk_rent` with the mean\n",
    "    rent_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    df['avg_2bhk_rent'] = rent_imputer.fit_transform(df[['avg_2bhk_rent']])\n",
    "\n",
    "    # Normalize `avg_2bhk_rent`\n",
    "    scaler = StandardScaler()\n",
    "    df['avg_2bhk_rent_scaled'] = scaler.fit_transform(df[['avg_2bhk_rent']])\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the train and test data\n",
    "train_dataset = preprocess_data(train_dataset)\n",
    "test_dataset = preprocess_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation and encoding:\n",
      " ID                        0\n",
      "area_type                 0\n",
      "availability              0\n",
      "location                  0\n",
      "size                      0\n",
      "society                   0\n",
      "total_sqft               40\n",
      "bath                      0\n",
      "balcony                   0\n",
      "price                     0\n",
      "dist_from_city            0\n",
      "avg_2bhk_rent             0\n",
      "total_sqft_min           40\n",
      "total_sqft_max           40\n",
      "dist_from_city_bin        0\n",
      "avg_2bhk_rent_missing     0\n",
      "avg_2bhk_rent_scaled      0\n",
      "dtype: int64\n",
      "Missing values after imputation and encoding:\n",
      " ID                       0\n",
      "area_type                0\n",
      "availability             0\n",
      "location                 0\n",
      "size                     0\n",
      "society                  0\n",
      "total_sqft               6\n",
      "bath                     0\n",
      "balcony                  0\n",
      "dist_from_city           0\n",
      "avg_2bhk_rent            0\n",
      "total_sqft_min           6\n",
      "total_sqft_max           6\n",
      "dist_from_city_bin       0\n",
      "avg_2bhk_rent_missing    0\n",
      "avg_2bhk_rent_scaled     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values after imputation and encoding:\\n\", train_dataset.isnull().sum())\n",
    "print(\"Missing values after imputation and encoding:\\n\", test_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Encoding_Data(df):\n",
    "# Custom Parsing for 'total_sqft'\n",
    "    def parse_total_sqft(value):\n",
    "        if '-' in str(value):\n",
    "            values = value.split('-')\n",
    "            return (float(values[0]) + float(values[1])) / 2\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    df['total_sqft_min'] = df['total_sqft_min'].apply(parse_total_sqft)\n",
    "    total_sqft_median = df['total_sqft_min'].median()\n",
    "    df['total_sqft_min'].fillna(total_sqft_median, inplace=True)\n",
    "\n",
    "    df['total_sqft'] = df['total_sqft'].apply(parse_total_sqft)\n",
    "    total_sqft_median = df['total_sqft'].median()\n",
    "    df['total_sqft'].fillna(total_sqft_median, inplace=True)\n",
    "    \n",
    "    df['total_sqft_max'] = df['total_sqft_max'].apply(parse_total_sqft)\n",
    "    total_sqft_median = df['total_sqft_max'].median()\n",
    "    df['total_sqft_max'].fillna(total_sqft_median, inplace=True)\n",
    "    # One-Hot Encoding for 'area_type'\n",
    "    df = pd.get_dummies(df, columns=['area_type'], prefix='area_type')\n",
    "\n",
    "    # Label Encoding for 'availability'\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['availability'] = le.fit_transform(df['availability'])\n",
    "\n",
    "    # Target Encoding for 'location' and 'society'\n",
    "    if target_encoder:\n",
    "        df[['location', 'society']] = target_encoder.transform(df[['location', 'society']])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create and fit the TargetEncoder on the train data\n",
    "target_encoder = TargetEncoder(cols=['location', 'society'])\n",
    "train_df[['location', 'society']] = target_encoder.fit_transform(train_df[['location', 'society']], train_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = test_Encoding_Data(train_dataset)\n",
    "test_dataset = test_Encoding_Data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv('train_dataset1.csv', index=False)\n",
    "test_dataset.to_csv('test_dataset1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation and encoding:\n",
      " ID                                0\n",
      "availability                      0\n",
      "location                          0\n",
      "size                              0\n",
      "society                           0\n",
      "total_sqft                        0\n",
      "bath                              0\n",
      "balcony                           0\n",
      "price                             0\n",
      "dist_from_city                    0\n",
      "avg_2bhk_rent                     0\n",
      "total_sqft_min                    0\n",
      "total_sqft_max                    0\n",
      "dist_from_city_bin                0\n",
      "avg_2bhk_rent_missing             0\n",
      "avg_2bhk_rent_scaled              0\n",
      "area_type_Built-up  Area          0\n",
      "area_type_Carpet  Area            0\n",
      "area_type_Plot  Area              0\n",
      "area_type_Super built-up  Area    0\n",
      "dtype: int64\n",
      "Missing values after imputation and encoding:\n",
      " ID                                0\n",
      "availability                      0\n",
      "location                          0\n",
      "size                              0\n",
      "society                           0\n",
      "total_sqft                        0\n",
      "bath                              0\n",
      "balcony                           0\n",
      "dist_from_city                    0\n",
      "avg_2bhk_rent                     0\n",
      "total_sqft_min                    0\n",
      "total_sqft_max                    0\n",
      "dist_from_city_bin                0\n",
      "avg_2bhk_rent_missing             0\n",
      "avg_2bhk_rent_scaled              0\n",
      "area_type_Built-up  Area          0\n",
      "area_type_Carpet  Area            0\n",
      "area_type_Plot  Area              0\n",
      "area_type_Super built-up  Area    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values after imputation and encoding:\\n\", train_dataset.isnull().sum())\n",
    "print(\"Missing values after imputation and encoding:\\n\", test_dataset.isnull().sum())\n",
    "\n",
    "#train_dataset.to_csv('trainnew.csv', index=False)\n",
    "#test_dataset.to_csv('testnew.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the test data has the same features as the train data\n",
    "#X_train = train_dataset.drop(columns=['price'])\n",
    "#y_train = train_dataset['price']\n",
    "X_test = test_dataset.drop(columns=['ID'])\n",
    "X = train_dataset.drop(columns=['ID', 'price'])\n",
    "y = train_dataset['price']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:  {'subsample': 0.8, 'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 7, 'learning_rate': 0.01}\n",
      "Best model score:  0.8141739338490053\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters distribution\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500, 1000],  # Number of boosting stages (trees)\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate\n",
    "    'max_depth': [3, 5, 7, 10],              # Max depth of individual trees\n",
    "    'subsample': [0.8, 0.9, 1.0],            # Fraction of samples used for fitting each tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=5, \n",
    "                                   scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters found\n",
    "print(\"Best hyperparameters found: \", random_search.best_params_)\n",
    "\n",
    "# Best model with the best hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best model score: \", best_model.score(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the price for the train data\n",
    "predictions = best_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 60.76825219807747\n"
     ]
    }
   ],
   "source": [
    "rmse_valid = np.sqrt(mean_squared_error(y_valid, predictions))\n",
    "print(f\"Validation RMSE: {rmse_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the price for the train data\n",
    "test_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predictions to the test DataFrame\n",
    "test_dataset['price'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_dataset[['ID', 'price']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
